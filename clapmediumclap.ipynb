{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/jaganadhg/clapmediumclap?scriptVersionId=88611497\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Introduction\nMedium is one of the leading digital publishing platforms. People from all the disciples started to publish their content on this platform. If a user is impressed by the material in a post, they can engage by adding a comment or expressing a clap. The current data-set contains details extracted from 6008 medium blog posts published under various publication banners on medium. The current task is to predict claps based on the article metadata. ","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport os\nimport unicodedata\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport sklearn as sl","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"medium_data = pd.read_csv(\"../input/medium-articles-dataset/medium_data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attribute Infromation\n\nThe data contains nine attributes, including 'id.'  The attributes are:\n* id: Unique id for each record \n* url: URL for the Medium post\n* title: title of the medium post \n* subtitle: subtitle of the medium post\n* image: image file name if available. The images are available in the image folder. \n* claps: total claps received fort the post. This is the target variable for our task here. \n* response: count of comments for the post.\n* reading_time: reading time estimated by Medium for the post. \n* publication: The publication name in Medium, such as 'Towards Data Science.' \n* date:  date of publication. This is just a date, not date and time. ","metadata":{}},{"cell_type":"code","source":"for idx in range(10):\n    print(medium_data.title[idx], medium_data.title[idx].split(\" \"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Potential Data Quality Issue**\n\nThere is a non-breaking space visible in the text. It may impact the tokenization efforts if applied in the text. ","metadata":{}},{"cell_type":"code","source":"def normalize_text(text : str) -> str:\n    \"\"\" Normalize the unicode string\n        :param text: text data\n        :retrns clean_text: clean text\n    \"\"\"\n    \n    if text != np.nan:\n        clean_text = unicodedata.normalize(\"NFKD\",\n                                           text)\n    else:\n        clean_text = text\n    \n    return clean_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data['clean_title'] = medium_data.title.apply(lambda x: normalize_text(x) if x!= np.nan else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.title[0], medium_data.clean_title[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data['clean_subtitle'] = medium_data.subtitle.apply(lambda x: normalize_text(x) if x!= np.nan and type(x) == str else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating New Features**\n\nWe will create two new features, namely the title word count (title_wc) and subtitle word count(subtitle_wc). ","metadata":{}},{"cell_type":"code","source":"def create_wc(text : str) -> int:\n    \"\"\" Count words in a text\n        :param text: String to check the len\n        :retirns wc: Word count\n    \"\"\"\n    \n    wc = 0\n    \n    norm_text = text.lower()\n    \n    wc = len(norm_text.split(\" \"))\n    \n    return wc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.title[0].lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data['title_wc'] = medium_data.title.apply(lambda x: create_wc(x) if x!= np.nan else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data['subtitle_wc'] = medium_data.subtitle.apply(lambda x: create_wc(x) if x!= np.nan and type(x) == str else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cout_pub_ax = medium_data.publication.value_counts().plot(kind='bar',\n                                                        figsize=(10,6),\n                                                        rot=35,\n                                                        align='center',\n                                                        title=\"Count of Article by Publication\")\ncout_pub_ax.set_xlabel(\"Publication\")\ncout_pub_ax.set_ylabel(\"Count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pub_clap_ax = medium_data.groupby(['publication'])['claps'].agg(sum).plot(kind='bar',\n                                                                           figsize=(10,6),\n                                                                           rot=35,\n                                                                          align='center',\n                                                                           title=\"Claps by Publications\")\npub_clap_ax.set_xlabel(\"Publication\")\npub_clap_ax.set_ylabel(\"Count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.title_wc.plot(kind='hist',\n                         figsize=(10,6),\n                         title=\"Histogram of Title Word Count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.subtitle_wc.plot(kind='hist',\n                         figsize=(10,6),\n                         title=\"Histogram of Sub Title Word Count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.reading_time.plot(kind='hist',\n                         figsize=(10,6),\n                         title=\"Histogram of Reading Time\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basline Model\nNow let's create a baseline model, nothing fancy yet. We will use the following attributes from the data and build a RandomForest model.  \n\n#### Attributes Selected \n * publication  \n * title_wc \n * subtitle_wc\n * reading_time\n * claps","metadata":{}},{"cell_type":"code","source":"model_data = medium_data[['publication','title_wc','subtitle_wc','reading_time','claps']]\nmodel_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The publication attribute is categorical, so we are applying one-hot encoding here. ","metadata":{}},{"cell_type":"code","source":"publications_cat = pd.get_dummies(model_data.publication)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reading time range from 1 to 50. From the graph, it is evident that time longer than 15 is very less. Lets clips the values here for uniformity. ","metadata":{}},{"cell_type":"code","source":"model_data.reading_time.clip(lower=1,upper=15,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_data.reading_time.plot(kind='hist',\n                         figsize=(10,6),\n                         title=\"Histogram of Reading Time after Clip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's drop the publication column and add the one-hot encoded values to the dataframe. ","metadata":{}},{"cell_type":"code","source":"#model_data.drop('publication',\n#                inplace=True,\n#               axis=1)\n#model_data.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_data_treated = pd.concat([publications_cat,model_data],\n                              axis=1,\n                               sort=False)\nmodel_data_treated.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are not building a classification model. But would love to include the behavior of each publication type in the model. We decided to stratify the data based on publication attributes and create a training test split by 70-30. ","metadata":{}},{"cell_type":"code","source":"train,test = train_test_split(model_data_treated,\n                              test_size=0.3,\n                             stratify=model_data_treated['publication'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will drop the publication attribute from the data (train and test). ","metadata":{}},{"cell_type":"code","source":"train.drop('publication',\n                inplace=True,\n               axis=1)\ntrain.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop('publication',\n                inplace=True,\n               axis=1)\ntest.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling Time!\nLet's build our baseline model here. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model = RandomForestRegressor()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = train[train.columns[:-1]]\ntrain_y = train[['claps']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = rf_model.fit(train_x,\n            train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x = test[test.columns[:-1]]\ntest_y = test[['claps']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = rf_model.predict(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"prediction\"] = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sl.metrics.mean_squared_error(test.claps, test.prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = test.claps.values\nfig, ax = plt.subplots(figsize=(10,6))\nax.scatter(y, predictions)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have a pretty awesome bleh! model :-(. Time to wear a strategy hat and work more!!!","metadata":{}},{"cell_type":"markdown","source":"## AutoML\n\nLet's try what automl can do here. We are using the libraray TPOT here","metadata":{}},{"cell_type":"code","source":"from tpot import TPOTRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"automl_reg = TPOTRegressor(generations=10,\n                          population_size=100,\n                          verbosity=2,\n                          random_state=2020,\n                          early_stop=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"automl_reg.fit(train_x,\n            train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"automl_predict = automl_reg.predict(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sl.metrics.mean_squared_error(test.claps, automl_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = test.claps.values\nfig, ax = plt.subplots(figsize=(10,6))\nax.scatter(y, automl_predict)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"automl_predict\"] = automl_predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medium_data.image.nunique","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}